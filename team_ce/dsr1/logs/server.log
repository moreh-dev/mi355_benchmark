INFO 11-24 05:46:35 [__init__.py:225] Automatically detected platform rocm.
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:38 [api_server.py:1876] vLLM API server version 0.11.1rc2.dev141+g38f225c2a
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:38 [utils.py:243] non-default args: {'model_tag': 'deepseek-ai/DeepSeek-R1-0528', 'host': 'localhost', 'port': 8888, 'model': 'deepseek-ai/DeepSeek-R1-0528', 'max_model_len': 16384, 'tensor_parallel_size': 8, 'block_size': 1, 'gpu_memory_utilization': 0.95, 'swap_space': 64.0, 'enable_prefix_caching': False, 'max_num_batched_tokens': 65536, 'max_num_seqs': 1024, 'async_scheduling': True}
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:39 [config.py:410] Replacing legacy 'type' key with 'rope_type'
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:52 [model.py:658] Resolved architecture: DeepseekV3ForCausalLM
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:52 [model.py:1745] Using max model len 16384
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:53 [arg_utils.py:1446] Defaulting to mp-based distributed executor backend for async scheduling.
[1;36m(APIServer pid=8)[0;0m INFO 11-24 05:46:53 [scheduler.py:225] Chunked prefill is enabled with max_num_batched_tokens=65536.
[1;36m(APIServer pid=8)[0;0m WARNING 11-24 05:46:53 [vllm.py:498] No piecewise cudagraph for executing cascade attention. Will fall back to eager execution if a batch runs into cascade attentions
INFO 11-24 05:46:57 [__init__.py:225] Automatically detected platform rocm.
[1;36m(EngineCore_DP0 pid=313)[0;0m INFO 11-24 05:47:04 [core.py:730] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=313)[0;0m INFO 11-24 05:47:04 [core.py:97] Initializing a V1 LLM engine (v0.11.1rc2.dev141+g38f225c2a) with config: model='deepseek-ai/DeepSeek-R1-0528', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-0528', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-0528, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['+rms_norm', '+silu_and_mul', '+quant_fp8', 'none', '+rms_norm', '+quant_fp8'], 'splitting_ops': [], 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL: 2>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=313)[0;0m WARNING 11-24 05:47:04 [multiproc_executor.py:750] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=313)[0;0m INFO 11-24 05:47:04 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_0ac6b442'), local_subscribe_addr='ipc:///tmp/c6a43e78-6bbc-42cc-9a43-3e1676f7326a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:06 [__init__.py:225] Automatically detected platform rocm.
INFO 11-24 05:47:14 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_fdd8a873'), local_subscribe_addr='ipc:///tmp/3fdca6f6-bc3d-46c4-a847-d6228dccd96f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:14 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_96e6c235'), local_subscribe_addr='ipc:///tmp/47461d4c-2583-482f-ab64-0e570549113d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:15 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_05cd801c'), local_subscribe_addr='ipc:///tmp/87df94d6-0e24-4411-a8f1-5a376f1e2bbe', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:15 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_6d73e7ee'), local_subscribe_addr='ipc:///tmp/e7b56ab2-b534-416d-aaa7-472607e6358e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:15 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_8f8019f2'), local_subscribe_addr='ipc:///tmp/3057b475-6b10-4133-9d32-b35526c6e275', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:15 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_cf30e6e2'), local_subscribe_addr='ipc:///tmp/1c706e60-7900-415a-941f-cc099c112201', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:15 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_48f0b894'), local_subscribe_addr='ipc:///tmp/ad8085cf-1d0a-40a9-af87-10cd058e6f6b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-24 05:47:15 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 25165824, 10, 'psm_588f92ac'), local_subscribe_addr='ipc:///tmp/003db2dc-67d5-4e73-8275-3df4052312ef', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 11-24 05:47:15 [pynccl.py:111] vLLM is using nccl==2.26.6
INFO 11-24 05:47:24 [shm_broadcast.py:313] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_13537a34'), local_subscribe_addr='ipc:///tmp/72efe324-7a21-4178-b5cc-4f39c820f325', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 11-24 05:47:25 [pynccl.py:111] vLLM is using nccl==2.26.6
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
INFO 11-24 05:47:26 [parallel_state.py:1325] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(Worker_TP1 pid=465)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP0 pid=464)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP2 pid=466)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP5 pid=469)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP3 pid=467)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP7 pid=471)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP6 pid=470)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP4 pid=468)[0;0m INFO 11-24 05:47:26 [gpu_model_runner.py:2843] Starting to load model deepseek-ai/DeepSeek-R1-0528...
[1;36m(Worker_TP1 pid=465)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP2 pid=466)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP1 pid=465)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP1 pid=465)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP5 pid=469)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP2 pid=466)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP2 pid=466)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP0 pid=464)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP5 pid=469)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP5 pid=469)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP3 pid=467)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP0 pid=464)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP0 pid=464)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP3 pid=467)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP3 pid=467)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP1 pid=465)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP2 pid=466)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP5 pid=469)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP0 pid=464)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP6 pid=470)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP4 pid=468)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP3 pid=467)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP4 pid=468)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP4 pid=468)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP6 pid=470)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP6 pid=470)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP7 pid=471)[0;0m INFO 11-24 05:47:27 [rocm.py:258] Using AITER MLA backend on V1 engine.
[1;36m(Worker_TP7 pid=471)[0;0m WARNING 11-24 05:47:27 [fp8.py:155] DeepGEMM backend requested but not available.
[1;36m(Worker_TP7 pid=471)[0;0m INFO 11-24 05:47:27 [fp8.py:170] Using Triton backend for FP8 MoE
[1;36m(Worker_TP4 pid=468)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP6 pid=470)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP7 pid=471)[0;0m WARNING 11-24 05:47:27 [compilation.py:874] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
[1;36m(Worker_TP1 pid=465)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP5 pid=469)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=464)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=467)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=466)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP4 pid=468)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP6 pid=470)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
[1;36m(Worker_TP7 pid=471)[0;0m INFO 11-24 05:47:28 [weight_utils.py:419] Using model weights format ['*.safetensors']
